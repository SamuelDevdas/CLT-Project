{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA Pair Generation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\switchdrive\\SyncVM\\MscIds Course Materials\\4th Semester\\1-CLT\\CLT-Project\\venv-stage3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU: NVIDIA GeForce RTX 3070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "if device == 0:\n",
    "    print(\"CUDA is available. Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded data with summaries:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qatar Petroleum ( QP) is targeting aggressive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kakrapar-3 is the first of India's 700 megawat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New US President Joe Biden took office this we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The slow pace of Japanese reactor restarts con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Two of New York City's largest pension funds s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             summary\n",
       "0  Qatar Petroleum ( QP) is targeting aggressive ...\n",
       "1  Kakrapar-3 is the first of India's 700 megawat...\n",
       "2  New US President Joe Biden took office this we...\n",
       "3  The slow pace of Japanese reactor restarts con...\n",
       "4  Two of New York City's largest pension funds s..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the summarized data\n",
    "input_file = \"summarized_cleantech_media_dataset.csv\"\n",
    "data_chunk = pd.read_csv(input_file)\n",
    "\n",
    "# Display the first few rows to confirm the data structure\n",
    "print(\"\\nLoaded data with summaries:\")\n",
    "data_chunk[[\"summary\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_chunk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.5fa34190089f0ee40f9cce3cafc396b89b2e5e83.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.5fa34190089f0ee40f9cce3cafc396b89b2e5e83.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.40it/s]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model and tokenizer\n",
    "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\switchdrive\\SyncVM\\MscIds Course Materials\\4th Semester\\1-CLT\\CLT-Project\\venv-stage3\\Lib\\site-packages\\transformers\\generation\\utils.py:1659: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated QA pairs saved to improved_media_qa_pairs_test.csv\n",
      "\n",
      "Generated QA pairs:\n",
      "                                             summary  \\\n",
      "0  Qatar Petroleum ( QP) is targeting aggressive ...   \n",
      "\n",
      "                                            question  \\\n",
      "0  \\n\\nQatar Petroleum ( QP) is targeting aggress...   \n",
      "\n",
      "                                              answer  \n",
      "0  Qatar Petroleum ( QP) is targeting aggressive ...  \n"
     ]
    }
   ],
   "source": [
    "def generate_question(context, max_length=70):\n",
    "    input_text = f\"\\n\\n{context}\\n-------------------\\n Generate only one question based on the above context and Just return the Question, nothing else\"\n",
    "    input_ids = tokenizer.encode(\n",
    "        input_text, return_tensors=\"pt\", max_length=512, truncation=True\n",
    "    )\n",
    "    outputs = model.generate(input_ids, max_new_tokens=max_length)\n",
    "    question = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return question\n",
    "\n",
    "\n",
    "# Generate QA pairs for a few rows to test the code\n",
    "test_data = data_chunk.head(1)  # Use only the first n rows for testing\n",
    "qa_pairs = []\n",
    "\n",
    "for index, row in test_data.iterrows():\n",
    "    summary = row[\"summary\"]\n",
    "    question = generate_question(summary)\n",
    "    qa_pairs.append({\"summary\": summary, \"question\": question, \"answer\": summary})\n",
    "\n",
    "# Save the generated QA pairs in the current directory\n",
    "qa_pairs_df = pd.DataFrame(qa_pairs)\n",
    "qa_pairs_file_path = \"improved_media_qa_pairs_test.csv\"\n",
    "qa_pairs_df.to_csv(qa_pairs_file_path, index=False)\n",
    "\n",
    "print(f\"\\nGenerated QA pairs saved to {qa_pairs_file_path}\")\n",
    "print(\"\\nGenerated QA pairs:\")\n",
    "print(qa_pairs_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Qatar Petroleum ( QP) is targeting aggressive cuts in its greenhouse gas emissions as it prepares to launch Phase 2 of its planned 48 million ton per year LNG expansion. The company is also aiming to reduce gas flaring intensity across its upstream facilities.\n",
      "-------------------\n",
      " Generate only one question based on the above context and Just return the Question, nothing else.\n",
      "\n",
      "- response: What are the key strategies Qatar Petroleum (QP) is implementing to aggressively cut greenhouse gas emissions and reduce gas flaring intensity in its Phase 2 LNG expansion and upstream facilities?\n"
     ]
    }
   ],
   "source": [
    "print(qa_pairs_df[\"question\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
